---
title: "R Notebook"
output: html_notebook
---
#Tennis predictions with tree-based models
Data can be found here: https://datahub.io/sports-data/atp-world-tour-tennis-data
First load the required packages
```{r message = FALSE}
#Load packages
library(tidyverse)
library(rpart)
library(rpart.plot)
```
I fixed the data in excel - each row was a match with both players, and all the stats. Needed a row per athlete with corresponding stats. Im sure there is a way to do this in R but in the interests of time I just did some copy/paste in excel. The data file can be found here.
```{r message = FALSE, warning = FALSE}
#Import
tennis_2017_results <- read_csv("match_stats_2017_unindexed_csv.csv")
tennis_2017_results$Result <- as.factor(tennis_2017_results$Result)
```
Now each row contains an athlete, so a match is spread across two rows.
I changed the 1 or 0 result to won or lost to make results easier to interpret. 
To make predictions, I need an outcome variable - in this case it is the Result column. However, if there are other variables directly correlated to the outcome, these should be removed. Variables such as sets won, total points won etc will be higher for the winner in most or all matches, therefore not useful. The current data set has 'total point won', 'total points total', there will be removed. I want to use match stats to predict outcome, not actual results. 
```{r}
#Remove the winner part of the colummn names string
colnames(tennis_2017_results)[2:26] <- substring(colnames(tennis_2017_results[,2:26]), 8)
colnames(tennis_2017_results)[26] <- 'Result'
#remove unwanted variables
tennis_2017_results <- select(tennis_2017_results, -total_points_won, -total_points_total)
#remove any missing data
tennis_2017_results <- tennis_2017_results[complete.cases(tennis_2017_results), ]
#check it out
head(tennis_2017_results)
```
There are other potentially problematic variables like 'Total points won by player 1' (TPW1) but ill leave them in for now. Also had to remove any rows with NAs as some of the models used later do not deal well with them. This removed around 52 rows from the 730. 

#Decision Tree
First model is a decision tree with all variables included.
```{r}
tennis_dt <- rpart(Result ~ ., 
                   data = tennis_2017_results,
                   method = 'class')
rpart.plot(tennis_dt)
```
This result is saying the number of break points converted is the most important statistic separating winners and losers in this data set with 78% of players winning when converting greater than 3 break points. From there, it appears that players who record >= 7 break points serve total win 95% of the time. Looks like break points are the defining factor in the outcome, however, decision trees aren't perfect and are prone to overfitting. 

To ensure we get less bias results we will try breaking the data into training and test sets. 

```{r}
set.seed(123) # set a random seed for reproducibility
tennis_2017_results$id <- 1:nrow(tennis_2017_results) #Create id col
train_set <- tennis_2017_results %>% sample_frac(.75)
test_set  <- anti_join(tennis_2017_results, train, by = 'id')
#Remove id col or it will be used in model
train_set <- select(train_set, -id)
test_set <- select(test_set, -id)
#Rerun the model
tennis_dt2 <- rpart(Result ~ ., 
                   data = train_set,
                   method = 'class')
rpart.plot(tennis_dt2)

```

Looks similar. But now we can test the prediction error on the test data set to find out how well the model predicts winners and losers.

```{r}
tennis_pred <- predict(object = tennis_dt2, # model object 
                            newdata = test_set, # test dataset
                            type = "class")  # return classification labels

library(caret)
# calculate the confusion matrix for the test set
caret::confusionMatrix(data = tennis_pred,                # predicted classes
                reference = test_set$Result)  # actual classes
```

The model correctly predicted a win 84% of the time (shown by specificity) and predicted a loss correctly 89% of the time (shown by sensitivity). Not bad. To try to improve we can try building the model with the caret package, which employs cross-validation. 

```{r}
#Auto tune with caret package
tennis_dt3 <- train(Result ~ ., 
                   data = train_set,
                   method = 'rpart')

tennis_pred3 <- predict(object = tennis_dt3,
                        newdata = test_set)

caret::confusionMatrix(data = tennis_pred3, 
                reference = test_set$Result)
```
Interestingly, prediction of winners decreased with the caret package auto tuning model. 

Time to ramp things up. Decision trees have known flaws related to overfitting and high variance. One way to overcomes these issues is to use a random forrest. 

#Random Forests

```{r}
library(randomForest)
tennis_rf <- randomForest(Result ~ ., 
                   data = train_set)
tennis_rf
```

In this case a loss was predicted correctly 90% of the time (shown here as the inverse - 10% error rate), and a win 91% of the time. Improved performance from the original decision tree. The out of bag error rate (OOB) was 9.71% - this is the error rate on samples not selected into the bootstrap training set. So no need for a training and test set, the funciton does this already, and the OOB error is the error rate on the 'test' set. Note, a big advantage of this is that you can use the entire dataset in the randomForest function. In this case it was trained on the training model so we can test accuracy on the test data. 

```{r}
tennis_pred4 <- predict(object = tennis_rf,  
                            newdata = test_set,  
                            type = "class")        

caret::confusionMatrix(data = tennis_pred4,          
                      reference = test_set$Result) 
auc(actual = test_set$Result, predicted = tennis_pred4)
```
Now we can see the advantage of using a random forest, the prediction on the test set has increased dramatically. The area under the curve (AUC) - another popular test of fit, shows strong results as well. We can try tuning the model to see what improvement, if any, it can achieve. 

This is done using TuneRF which allows us to train numerous models with different mtry values, minimising the OOB. Mtry adjusts the number of predictor variables randomly sampled as candidates at each split (adjusting the variability/randomness going into the model).

```{r}
tennis_tune <- tuneRF(x = train_set[,1:23],
              y = train_set$Result,
              ntreeTry = 1000)
tennis_tune
```

```{r}
#Get the final mtry value
mtry_opt <- tennis_tune[,"mtry"][which.min(tennis_tune[,"OOBError"])]
print(mtry_opt)
```
We can use this method to pick the best model (in this case, the one that uses an mtry value of 8) by selecting `doBest = TRUE` in `tuneRF()`. This will return the best model, instead of the results.

An alternate (and more generalisable) method for tuning hyper-parameters is with the caret package. These methods can be used for various machine learning models, as long as you know the hyper-parameters to tune. For more info see this resource I used for this section: https://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/

```{r}
control <- trainControl(method="repeatedcv", number=5, search="grid")
rf_gridsearch <- train(Result~., data=train_set, method="rf", metric='Accuracy', tuneGrid=expand.grid(.mtry=c(1:20)), trControl=control)
print(rf_gridsearch)
plot(rf_gridsearch)
```

This is very slow, so there is a trade-off between accuracy and time. Using caret cross validation shows an mtry between 5 and 20 performs fairly similarly in accuracy. Note, this is a good process to go through, though the randomForest function uses reasonable defaults - for mtry the default for a classification tree is sqrt(p) where p is number of variables. The exact code is floor(sqrt(ncol(x))), which results in an mtry of 4 for our data. In this case I would re-train the modlel, specifying the mtry.

#Boosted tree
The last type of model we will try is a boosted tree, specifically the gradient boosting machine (GBM). This is fairly easy to implement and if tuned corretly can perform better than just about any other model. 

```{r}
library(gbm)
tennis_gbm <- gbm(formula = Result ~ ., 
               distribution = "bernoulli",
               data = train_set,
               n.trees = 1000)

tennis_gbm
summary(tennis_gbm)
```

This provides a list of influence for each predictor in the dataset. Clearly break points converted and break points serve total have a major influence on the model's predictions, followed by service points won, break points saved and many others with similarly low influence.

Finally, lets see how the gbm will perform on the test set. 
```{r}
library(ModelMetrics)
tennis_pred5 <- predict(object = tennis_gbm,  
                            newdata = test_set,  
                            type = "response",
                            n.trees = 1000)  

auc(actual = test_set$Result, predicted = tennis_pred5)  
```
As we can see, the boosted model improved AUC, but only very slightly. With strong predictors identified in all models we have made - relating to break points - there have only been marginal improvements. 


